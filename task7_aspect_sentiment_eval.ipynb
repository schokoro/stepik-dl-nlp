{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аспектный анализ тональности текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] https://github.com/nlpub/pymystem3\n",
    "* [2] SentiRuEval2015\n",
    "* [3] https://rusvectores.org/ru/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git\n",
    "# import sys; sys.path.append('/content/stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оригинальная разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab, добавьте в начало пути /content/stepik-dl-nlp\n",
    "xml_sentiments = 'datasets/sentirueval2015/SentiRuEval_car_markup_train.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import parse_xml_sentiment, parse_xml_aspect, show_markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тексты с разметкой аспектов и тональностей: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_w_sentiment_spans = parse_xml_sentiment(xml_sentiments)\n",
    "texts_w_aspect_spans    = parse_xml_aspect(xml_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = len(texts_w_sentiment_spans)\n",
    "\n",
    "print('Загружено {} текстов с разметкой тональности\\n'.format(amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### выберем 2 текста, на которых будем рассматривать все примеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_picks = [random.randint(0,amount-1) for _ in range(0,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  тональность (sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rand_i in random_picks:\n",
    "    text, spans = texts_w_sentiment_spans[rand_i]\n",
    "    \n",
    "    print('Текст №:',rand_i)\n",
    "    show_markup(text,spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Аспекты (aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rand_i in random_picks:\n",
    "    text,spans = texts_w_aspect_spans[rand_i]\n",
    "    \n",
    "    print('Текст №:',rand_i)\n",
    "    show_markup(text,spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO-тэги для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import fill_gaps, extract_BIO_tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rand_i in random_picks:\n",
    "    text, aspect_spans = texts_w_aspect_spans[rand_i]\n",
    "    cover_spans       = fill_gaps(text, aspect_spans)\n",
    "    \n",
    "    print('Полное покрытие разметкой текста №:',rand_i) \n",
    "    show_markup(text, cover_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение текста на предложения, а предложений - на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import regex_sentence_detector, sentence_spans,sentence_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "word_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rand_i in random_picks:\n",
    "    text, aspect_spans = texts_w_aspect_spans[rand_i]\n",
    "\n",
    "    print('Разбиение на предложения и BIO токенизация текста №:',rand_i) \n",
    "    for sentence, spans in sentence_splitter(text, aspect_spans):\n",
    "\n",
    "        cover_spans      = fill_gaps(sentence,spans)\n",
    "        tokens_w_biotags = extract_BIO_tagged_tokens(sentence, \n",
    "                                                     cover_spans, \n",
    "                                                     word_tokenizer.tokenize)\n",
    "\n",
    "        show_markup(sentence, cover_spans)\n",
    "        print(tokens_w_biotags[:10],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Подготовка данных для обучения: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import prepare_data, form_vocabulary_and_tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab, добавьте в начало пути /content/stepik-dl-nlp\n",
    "xml_train = 'datasets/sentirueval2015/SentiRuEval_car_markup_train.xml'\n",
    "xml_test  = 'datasets/sentirueval2015/SentiRuEval_car_markup_test.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_w_aspect_spans = parse_xml_aspect(xml_train)\n",
    "training_data        = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n",
    "\n",
    "texts_w_aspect_spans = parse_xml_aspect(xml_test)\n",
    "test_data            = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### разбиение на предложения дало нам столько коротких текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = training_data + test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary,labels = form_vocabulary_and_tagset(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### а размер словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### индексация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import Converter, generate_markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = Converter(vocabulary,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe, test_tags = training_data[1211]\n",
    "\n",
    "text, spans = generate_markup(test_recipe, test_tags) \n",
    "\n",
    "show_markup(text, spans)\n",
    "\n",
    "encoded_recipe = converter.words_to_index(test_recipe)\n",
    "encoded_tags   = converter.tags_to_index(test_tags)\n",
    "\n",
    "print(encoded_recipe)\n",
    "print(encoded_tags)\n",
    "print()\n",
    "\n",
    "decoded_recipe = converter.indices_to_words(encoded_recipe)\n",
    "decoded_tags   = converter.indices_to_tags(encoded_tags)\n",
    "\n",
    "text, spans = generate_markup(decoded_recipe, decoded_tags) \n",
    "\n",
    "show_markup(text, spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM    = 32\n",
    "VOCAB_SIZE    = len(converter.word_to_idx)\n",
    "TAGSET_SIZE   = len(converter.tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Предобученные вектора слов\n",
    "\n",
    "Алгоритм fastText обученный на корпусе Тайга, смотрите подробности на сайте: https://rusvectores.org/ru/models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gensim\n",
    "import wget\n",
    "\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/187.zip'\n",
    "wget.download(model_url)\n",
    "model_file = 'datasets/' + model_url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load('datasets/187/model.model')\n",
    "\n",
    "words = ['тачка', 'двигатель', 'ауди']\n",
    "\n",
    "for word in words:\n",
    "    if word in w2v_model:\n",
    "        print(word)\n",
    "        \n",
    "        for i in w2v_model.most_similar(positive=[word], topn=10):\n",
    "            nearest_word      = i[0]\n",
    "            cosine_similarity = i[1]\n",
    "            print(nearest_word, cosine_similarity)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_embeddings = np.zeros(shape=[VOCAB_SIZE, EMBEDDING_DIM],dtype=np.float32)\n",
    "\n",
    "for word in vocabulary:\n",
    "    if word in w2v_model:\n",
    "        index  = converter.words_to_index([word])\n",
    "        vector = w2v_model.get_vector(word)\n",
    "        \n",
    "        numpy_embeddings[index] = vector\n",
    "        \n",
    "    else:\n",
    "        print(word + ' - такого слова нет в модели fasttext')\n",
    "        \n",
    "pretrained_embeddings = torch.FloatTensor(numpy_embeddings)\n",
    "pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM \n",
    "\n",
    "1. Использует предобученные вектора слов и не изменяет их\n",
    "2. Двунаправленная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_embeddings):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim      = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        self.lstm            = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag      = nn.Linear(2*hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, words):\n",
    "        embeds      = self.word_embeddings(words)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(words), 1, -1))\n",
    "        tag_space   = self.hidden2tag(lstm_out.view(len(words), -1))\n",
    "        tag_scores  = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "    def predict_tags(self, words):\n",
    "        with torch.no_grad():\n",
    "            tags_pred = model(words).numpy()\n",
    "            tags_pred = np.argmax(tags_pred, axis=1)\n",
    "            \n",
    "        return tags_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Взвешеная функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_counter = Counter()\n",
    "for _,tokens in training_data:\n",
    "    for token in tokens:\n",
    "        tag_counter[token]+=1\n",
    "        \n",
    "tag_counter.most_common()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.ones(15)\n",
    "class_divs    = torch.ones(15)\n",
    "\n",
    "for tag, inv_weight in tag_counter.most_common():\n",
    "    tag_idx             = converter.tags_to_index([tag])\n",
    "    class_divs[tag_idx] = inv_weight\n",
    "    \n",
    "norm       = torch.norm(class_divs, p=2, dim=0).detach()\n",
    "class_divs = class_divs.div(norm.expand_as(class_divs))\n",
    "\n",
    "\n",
    "class_weights /= class_divs\n",
    "\n",
    "\n",
    "print(class_weights.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model         = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE, pretrained_embeddings)\n",
    "loss_function = nn.NLLLoss(class_weights) \n",
    "optimizer     = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "liveplot = PlotLosses()\n",
    "\n",
    "for epoch in range(10): \n",
    "    for i, (recipe, tags) in enumerate(training_data):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        encoded_recipe = converter.words_to_index(recipe) # слово -> его номер в словаре \n",
    "        encoded_tags   = converter.tags_to_index(tags)    # тэг   -> его номер в списке тэгов\n",
    "        tag_scores     = model(encoded_recipe)\n",
    "        \n",
    "        loss = loss_function(tag_scores, encoded_tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            liveplot.update({'negative log likelihood loss': loss})\n",
    "            liveplot.draw()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, converter, recipe):\n",
    "    \n",
    "    encoded_recipe = converter.words_to_index(recipe)        # слово -> его номер в словаре\n",
    "\n",
    "    encoded_tags   = model.predict_tags(encoded_recipe)      # предсказанные тэги (номера)\n",
    "\n",
    "    decoded_tags   = converter.indices_to_tags(encoded_tags) # номер тэга -> тэг\n",
    "    \n",
    "    return decoded_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "\n",
    "    recipe, tags = test_data[np.random.randint(0,1000)]\n",
    "    \n",
    "    tags_pred    = predict_tags(model, converter, recipe)\n",
    "\n",
    "    print('истинные тэги:')\n",
    "    text, spans = generate_markup(recipe, tags) \n",
    "    show_markup(text, spans)\n",
    "\n",
    "    print('предсказанные тэги:')\n",
    "    text, spans = generate_markup(recipe, tags_pred) \n",
    "\n",
    "    show_markup(text, spans)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Статистика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Количество верно предсказанных тэгов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import tag_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_correct, total_tags = tag_statistics(model, converter, test_data)\n",
    "\n",
    "\n",
    "print('Статистика верно предсказанных тэгов:\\n')\n",
    "\n",
    "for tag in total_tags.keys():\n",
    "    print('для {}:'.format(tag))\n",
    "    print('  корректно:\\t', total_correct[tag])\n",
    "    print('      всего:\\t',   total_tags[tag])\n",
    "    print('% корректно:\\t', 100 * (total_correct[tag] / float(total_tags[tag])))\n",
    "    print()\n",
    "\n",
    "print('----------')\n",
    "print('в итоге:')\n",
    "print('  корректно:\\t', sum(total_correct.values()))\n",
    "print('      всего:\\t', sum(total_tags.values()))\n",
    "print('% корректно:\\t', 100 * (sum(total_correct.values()) / sum(total_tags.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.  Количество верно предсказанных тэгов в виде матрицы ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlnlputils.sentiment_utils import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for sentence, tags in test_data:\n",
    "    y_pred += predict_tags(model, converter, sentence)\n",
    "    y_true += tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred, classes=list(total_tags.keys()), title='Матрица ошибок')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred, classes=list(total_tags.keys()), normalize=True, \n",
    "                      title='Нормализованная матрица ошибок')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
